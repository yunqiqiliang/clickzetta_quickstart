{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "k5Mvn0qyNlF7"
   },
   "source": [
    "# 在云器Lakehouse的同一张表中进行向量和标量存储\n",
    "\n",
    "## 方案说明\n",
    "\n",
    "云器Lakehouse的github_event_issuesevent_embedding表里保存了文本字段，需要对该文本字段进行向量化，并保存在同一张表里的向量字段里，方便进行向量和标量的融合检索。\n",
    "\n",
    "方案实现了同一张表、同一个VCluster，同时支持了文本数据、向量数据以及倒排索引和向量索引的存储。和传统方式相比，不再需要三套系统（数据仓库、文本检索数据库、向量数据库），最大程度了降低了数据副本数量，避免了数据在三套系统之间的同步。\n",
    "\n",
    "\n",
    "![image.png](./image/scala_vector_store_in_one_table.png)\n",
    "\n",
    "- 用到的云器Lakehouse的关键Feature：\n",
    "  - 向量存储：原生的Vector数据类型，在普通表里直接增加Vector类型的字段；\n",
    "  - 向量索引：对Vector类型的字段建立索引，加速向量检索的速度；\n",
    "  - 倒排索引：对文本字段建立倒排索引，加速文本检索的速度；\n",
    "  - bloom_filter索引：对ID字段建立索引，加速ID过滤；\n",
    "- 模型服务\n",
    "  - xinference，本地部署xinference，提供embedding和rerank模型服务；\n",
    "  - 本方案采用1024维的向量化表示；\n",
    "- 测试数据集介绍\n",
    "  - 数据来源是Github的IssuesEvent事件，全文检索字段为其中的issue_body；\n",
    "  - 向量化表示的是issue_body对应的向量化数据；\n",
    "  - 全表有1.9亿条记录；\n",
    "\n",
    "\n",
    "## 数据说明\n",
    "\n",
    "- 表名：github_event_issuesevent_embedding\n",
    "- 文本字段：issue_body, string类型\n",
    "- 向量化字段：issue_body_embedding,vector(float,1024)类型\n",
    "- 向量化方法：issue_body_embedding的初始值为NULL。调用xinference/ollama本地服务，用bge-m3模型对issue_body字段的文本进行向量化，然后保存在issue_body_embedding字段\n",
    "\n",
    "\n",
    "## issue_body_embedding字段更新方法\n",
    "- 单条update方法\n",
    "  - 符合传统数据库开发者的习惯，数据更实时，达到秒级数据新鲜度高。但是在大数据平台上用SQL进行频繁的update，会带来明显的弊端：\n",
    "  - 带来大量的小文件需要及时进行合并，以优化性能\n",
    "  - 需要一直启动VCluster，造成计算成本高\n",
    "- 批量merge into方法\n",
    "  - 牺牲了数据新鲜度，从秒级到分钟级\n",
    "  - 规避了小文件急剧增多的问题\n",
    "  - 大幅降低了计算成本，每5分钟merge into一次，则计算成本下降幅度达到80%，大幅提升了数据向量化的性价比。这对大数据量的向量化非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本的向量化表示，xinference\n",
    "bge-m3为1024维，bge-base-zh-v1.5为768维\n",
    "本文通过私有部署xinference，提供向量embedding模型服务和rerank召回模型服务 xinference运行在X86 CPU上, xinference的安装和使用请参考这里的[文档](https://inference.readthedocs.io/en/latest/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的嵌入向量维度：1024\n"
     ]
    }
   ],
   "source": [
    "from xinference.client import Client as Xinference_Client  # 添加别名\n",
    "\n",
    "def get_embedding_xin(\n",
    "    input_text: str,\n",
    "    base_url: str = \"http://localhost:9998\",\n",
    "    model_name: str = \"bge-m3\"\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    获取文本的嵌入向量\n",
    "    \n",
    "    参数:\n",
    "    input_text (str): 要生成嵌入向量的文本\n",
    "    base_url (str): Xinference服务器地址，默认为本地服务\n",
    "    model_name (str): 要使用的模型名称，默认为bge-m3\n",
    "    \n",
    "    返回:\n",
    "    list: 文本的嵌入向量\n",
    "    \"\"\"\n",
    "    # 使用别名创建客户端连接\n",
    "    client = Xinference_Client(base_url)  # 修改类名调用\n",
    "    \n",
    "    # 获取指定模型\n",
    "    model = client.get_model(model_name)\n",
    "    embedding = model.create_embedding(input_text)\n",
    "    # 生成并返回嵌入向量\n",
    "    return embedding['data'][0]['embedding']\n",
    "\n",
    "# 使用示例保持不变\n",
    "if __name__ == \"__main__\":\n",
    "    embedding = get_embedding_xin(\"What is the capital of China?\")\n",
    "    print(f\"生成的嵌入向量维度：{len(embedding)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本的向量化表示，ollama\n",
    "文本的向量化表示，1024维embedding函数 ollama运行在Mac M1 ARM上, ollama的安装和使用请参考这里的[文档](https://ollama.com/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量维度: 1024\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "def get_embedding_ollama(text: str, \n",
    "                 model: str = 'bge-m3',  # 默认模型\n",
    "                 host: str = 'http://192.168.6.167:11434') -> list[float]:\n",
    "    \"\"\"\n",
    "    获取文本的向量化表示\n",
    "    \n",
    "    参数：\n",
    "    text (str): 需要向量化的文本内容\n",
    "    model (str): 使用的embedding模型名称，默认为深度求索的embedding模型\n",
    "    host (str): Ollama服务器地址，格式为http://IP:PORT\n",
    "    \n",
    "    返回：\n",
    "    list[float]: 文本的向量表示（浮点数列表）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = Client(host=host)\n",
    "        response = client.embed(\n",
    "            model=model,\n",
    "            input=text.strip()  # 去除首尾空白字符\n",
    "        )\n",
    "        return response['embeddings'][0]\n",
    "    except Exception as e:\n",
    "        print(f\"获取embedding失败: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    embedding = get_embedding_ollama(\n",
    "        text=\"为什么天空是蓝色的？\"\n",
    "    )\n",
    "    \n",
    "    if embedding is not None:\n",
    "        print(f\"向量维度: {len(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 安装云器Zettapark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tVqlAMGIEHTT"
   },
   "outputs": [],
   "source": [
    "# !pip install -U clickzetta-zettapark-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1G1RE5iPNpfs"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1ijF-sgJNf-U"
   },
   "source": [
    "## 通过ZettaPark连接到云器Lakehouse(& without PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPtS8X-WEsz6",
    "outputId": "17bc376e-73f5-4db7-8813-6f883df8b573"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from clickzetta.zettapark.session import Session\n",
    "import clickzetta.zettapark.functions as f\n",
    "from clickzetta.zettapark import Session, DataFrame\n",
    "from clickzetta.zettapark.functions import udf, col\n",
    "from clickzetta.zettapark.types import IntegerType\n",
    "import clickzetta.zettapark.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在连接到云器Lakehouse.....\n",
      "\n",
      "连接成功！...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from clickzetta.zettapark.session import Session\n",
    "import logging\n",
    "logging.getLogger(\"clickzetta.zettapark\").setLevel(logging.ERROR)\n",
    "\n",
    "# 从配置文件中读取参数\n",
    "with open('config-vector.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "print(\"正在连接到云器Lakehouse.....\\n\")\n",
    "\n",
    "# 创建会话\n",
    "session = Session.builder.configs(config).create()\n",
    "\n",
    "\n",
    "print(\"连接成功！...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看计算资源详细情况\n",
    "session.sql(f\"desc vcluster extended {config['vcluster']}\").to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从云器Lakehouse表中查询待向量化的文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_date = \"2024-06-01\"\n",
    "end_date = \"2024-06-02\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_embedded = session.sql(f\"SELECT row_id, partition_date,issue_body from github_event_issuesevent_embedding where partition_date >= '{begin_date}' and partition_date <= '{end_date}' and issue_body_embedding is NULL LIMIT 100000;\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49312"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_be_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存向量数据 \n",
    "### 方法一：将文本数据向量化后merge into进表里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n",
      "issue_body_embedding_updates len = 500\n"
     ]
    }
   ],
   "source": [
    "# 定义批量大小\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "max_workers = 6\n",
    "\n",
    "batch_size = 500\n",
    "batches = []\n",
    "\n",
    "updatesSchema = T.StructType(\n",
    "    [\n",
    "        T.StructField(\"row_id\", T.IntegerType()),\n",
    "        T.StructField(\"partition_date\", T.StringType()),\n",
    "        T.StructField(\"issue_body_embedding\", T.StringType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 将待处理数据分批\n",
    "for i in range(0, len(to_be_embedded), batch_size):\n",
    "    batch = to_be_embedded[i:i + batch_size]\n",
    "    batches.append(batch)\n",
    "\n",
    "# 定义一个函数用于处理单条记录的嵌入生成\n",
    "def process_row(row, use_alternate_method=False):\n",
    "    \"\"\"\n",
    "    处理单条记录，生成对应的嵌入。\n",
    "    如果 use_alternate_method 为 True，则调用 get_embedding_ollama。\n",
    "    \"\"\"\n",
    "    row_id = row[\"row_id\"]\n",
    "    issue_body_text = row[\"issue_body\"]\n",
    "    partition_date = row[\"partition_date\"]\n",
    "\n",
    "    try:\n",
    "        # 根据标志调用不同的嵌入生成函数\n",
    "        if use_alternate_method:\n",
    "            issue_body_embedding = get_embedding_ollama(issue_body_text)\n",
    "        else:\n",
    "            issue_body_embedding = get_embedding_xin(issue_body_text)\n",
    "\n",
    "        return (row_id, partition_date, issue_body_embedding)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for row {row_id} using {'ollama' if use_alternate_method else 'xin'}: {e}\")\n",
    "        return None\n",
    "for batch in batches:\n",
    "    try:\n",
    "        # 使用 ThreadPoolExecutor 进行并发嵌入生成\n",
    "        update_rows = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "            for idx, row in enumerate(batch):\n",
    "                # 每隔一条记录切换到 get_embedding_ollama\n",
    "                use_alternate_method = (idx % 2 == 1)\n",
    "                futures.append(executor.submit(process_row, row, use_alternate_method))\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    update_rows.append(result)\n",
    "\n",
    "        # 将生成的结果保存到临时表\n",
    "        if update_rows:\n",
    "            update_df = pd.DataFrame(update_rows, columns=[\"row_id\", \"partition_date\", \"issue_body_embedding\"])\n",
    "            try:\n",
    "                zetta_update_df = session.create_dataframe(update_df, schema=updatesSchema)\n",
    "                zetta_update_df.write.mode(\"overwrite\").save_as_table(\"issue_body_embedding_updates\")\n",
    "            except Exception as save_error:\n",
    "                print(f\"Error saving batch to table: {save_error}\")\n",
    "    except Exception as batch_error:\n",
    "        print(f\"Error processing batch: {batch_error}\")\n",
    "        \n",
    "    issue_body_embedding_updates_len =  session.table(\"issue_body_embedding_updates\").count()\n",
    "    print(f\"issue_body_embedding_updates len = {issue_body_embedding_updates_len}\")\n",
    "    \n",
    "    merge_query = \"\"\"\n",
    "                MERGE INTO github_event_issuesevent_embedding AS target\n",
    "                USING issue_body_embedding_updates AS source\n",
    "                ON target.partition_date = source.partition_date\n",
    "                AND target.row_id = source.row_id\n",
    "                WHEN MATCHED THEN\n",
    "                  UPDATE SET issue_body_embedding = cast(source.issue_body_embedding as vector(1024))\n",
    "                \"\"\"\n",
    "    try:\n",
    "        session.sql(merge_query).collect()\n",
    "    except Exception as merge_error:\n",
    "        print(f\"Error during MERGE operation: {merge_error}\")\n",
    "        \n",
    "try:\n",
    "    session.sql(\"DROP TABLE IF EXISTS issue_body_embedding_updates\").collect()\n",
    "except Exception as drop_error:\n",
    "    print(f\"Error dropping temporary table: {drop_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：将文本数据向量化后update进表里（不建议使用此方法，原因建方案介绍）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 循环处理每一行需要生成嵌入的内容\n",
    "for row in to_be_embedded:\n",
    "    try:\n",
    "        # 获取当前行的元数据\n",
    "        row_id = row[\"row_id\"]\n",
    "        issue_body_text = row[\"issue_body\"]\n",
    "        partition_date = row[\"partition_date\"]\n",
    "        \n",
    "        # 调用嵌入生成函数\n",
    "        issue_body_embedding = get_embedding_xin(issue_body_text)\n",
    "        \n",
    "        # 执行SQL更新\n",
    "        update_stmt = f\"\"\"\n",
    "            USE VCLUSTER DEFAULT;\n",
    "            UPDATE github_event_issuesevent_embedding\n",
    "            SET issue_body_embedding = cast({issue_body_embedding} as vector(1024))\n",
    "            WHERE partition_date = \"{partition_date}\" and row_id = {row_id}\n",
    "        \"\"\"\n",
    "        session.sql(update_stmt).collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {row_id}: {str(e)}\")\n",
    "        # 可以根据需要添加重试逻辑或记录错误\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下一步\n",
    "02在云器Lakehouse的同一张表中进行向量和标量检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
