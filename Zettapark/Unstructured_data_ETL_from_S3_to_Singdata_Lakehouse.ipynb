{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sGT7Sjb93Zt6",
   "metadata": {
    "id": "sGT7Sjb93Zt6"
   },
   "source": [
    "# Transforming Unstructured Data from an AWS S3 bucket into RAG-Ready Data in Singdata Lakehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea21043",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aab86273",
   "metadata": {},
   "source": [
    "### Unified Processing of Unstructured and Structured Data in a Lakehouse for RAG Applications\n",
    "\n",
    "Developing Retrieval-Augmented Generation (RAG) applications within a Lakehouse architecture presents specific challenges. Integrating diverse data types like text, images, videos, and structured tables requires a robust and flexible architecture. Ensuring data quality and consistency across different formats and sources necessitates comprehensive validation and transformation processes. Managing and storing large volumes of unstructured and structured data efficiently while maintaining scalability and performance is a significant challenge. The processing and analysis of these data types together demand advanced algorithms and substantial computing power. Additionally, robust data governance, security, and compliance across various data types and sources add to the complexity.\n",
    "\n",
    "Despite these challenges, the unified processing of unstructured and structured data is essential for RAG application development. This approach enables the integration of diverse data types, providing a holistic view and uncovering deeper insights that might not be apparent when analyzing data separately. A unified approach streamlines operations, reducing the need for multiple systems, thus simplifying maintenance and lowering operational costs. It ensures data consistency and accuracy, improving the reliability of data-driven decisions. Unified processing allows for advanced analytics, combining insights from different data types for comprehensive and actionable insights. This approach optimizes resource utilization, enhances scalability and flexibility, simplifies architecture, and improves data consistency by reducing data duplication and movement. Furthermore, a simplified architecture with unified processing reduces operational overhead, improves development efficiency, and enhances data consistency, making it crucial for effective and streamlined RAG applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb6079",
   "metadata": {},
   "source": [
    "\n",
    "### Unified Data Pipeline Solution Overview\n",
    "\n",
    "**Data Source:**\n",
    "- Unstructured files on AWS S3 (PDF, Email, JPG, etc.)\n",
    "\n",
    "**AI Data Transformation:**\n",
    "- Transform unstructured data into JSON format, including embeddings, text summaries, and image summaries.\n",
    "\n",
    "**Data Load into Singdata Lakehouse:**\n",
    "- Load raw data into the `raw_table`, storing various metadata and content related to files and elements.\n",
    "\n",
    "**Data Clean and Transform (Singdata Lakehouse Vector/Inverted INDEX):**\n",
    "- Clean and transform raw data into the `silver_table` with vector index and inverted index.\n",
    "\n",
    "**Data Retrieval (Singdata Lakehouse SQL):**\n",
    "- Perform vector and text searches using Singdata Lakehouse SQL to retrieve and analyze data.\n",
    "\n",
    "\n",
    "![Image Alt Text](./image/UnstructuredDataPipelineSingdata.png)\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**AWS S3:** Store unstructured data\n",
    "\n",
    "**Unstructured:** Ingesting Unstructured Data from S3, Transform unstructured data into JSON format\n",
    "\n",
    "**Unstructured Singdata Lakehouse Connector:** Load data into Singdata Lakehouse\n",
    "\n",
    "**Singdata Lakehouse:** Store and manage transformed data for RAG Application with Vector Index and Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e15d87f040227",
   "metadata": {
    "id": "138e15d87f040227"
   },
   "source": [
    "\n",
    "In this quick tutorial we'll ingest PDFs/Emails/Images from an S3 bucket in same directory, transform them into a normalized JSON with Unstructured, which we will then chunk, embed and load into Singdata Lakehouse table.\n",
    "\n",
    "Then RAG application could retrieve the data in Singdata Lakehouse and get the embeddings and the text/graph content in a format that is ready to be used in a RAG application.\n",
    "\n",
    "### Prerequisites:\n",
    "\n",
    "A. Get your [Unstructured Serverless API key](https://www.google.com/url?q=https%3A%2F%2Funstructured.io%2Fapi-key-hosted). It comes with a 14-day trial, and a cap of 1000 pages/day.\n",
    "\n",
    "B. Get your [Singdata Lakehouse Account](https://singdata.com/). It comes with 1 month trial and ï¿¥200 coupons.\n",
    "\n",
    "C. Create an AWS S3 bucket, and populate it with PDFs of choice. Make sure to note down your credentials.\n",
    "\n",
    "D. Install the necessary libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:42:15.075048Z",
     "start_time": "2024-07-03T13:41:09.778849Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "8fcc1c00-faaa-4f35-a8e0-8f4eb50194b2"
   },
   "source": [
    "1, Open a terminal and create new Python3.9.21 environment:unstructured\n",
    "\n",
    "conda create -n unstructured python=3.9\n",
    "\n",
    "conda activate unstructured\n",
    "\n",
    "Then select unstructured as current environment\n",
    "\n",
    "2, You could contact with qiliang@clickzetta.com to get unstructured_ingest-0.5.5-py3-none-any.whl.\n",
    "\n",
    "!pip install -U dist/unstructured_ingest-0.5.5-py3-none-any.whl --force-reinstall\n",
    "\n",
    "!pip install -U \"unstructured-ingest[s3, pdf, clickzetta, embed-huggingface]\"\n",
    "\n",
    "!pip install --force-reinstall \"unstructured-ingest[clickzetta]\"\n",
    "\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e16887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, force=True)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# if you want to drop the tables before write data, set drop_tables to True\n",
    "drop_tables = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uNB1tW_F4WYk",
   "metadata": {
    "id": "uNB1tW_F4WYk"
   },
   "source": [
    "### Load env variables\n",
    "\n",
    "In this example we're loading the environment variables with all the secrets from a file in Localfile. The .evn file includes the following variables:\n",
    "\n",
    "cz_username: Username for connecting to the Lakehouse service \n",
    "\n",
    "cz_password: Password for connecting to the Lakehouse service\n",
    "\n",
    "cz_service: Name of the Lakehouse service to connect to\n",
    "\n",
    "cz_instance: Instance name of the Lakehouse service to connect to\n",
    "\n",
    "cz_workspace: Workspace name of the Lakehouse service to connect to\n",
    "\n",
    "cz_schema: Schema name of the Lakehouse service to connect to\n",
    "\n",
    "cz_vcluster: Virtual cluster name of the Lakehouse service to connect to\n",
    "\n",
    "AWS_KEY: Key for connecting to AWS services\n",
    "\n",
    "AWS_SECRET: Secret key for connecting to AWS services\n",
    "\n",
    "AWS_S3_NAME: Bucket name for connecting to AWS S3 service\n",
    "\n",
    "UNSTRUCTURED_API_KEY: API key for connecting to the UNSTRUCTURED API\n",
    "\n",
    "UNSTRUCTURED_URL: URL for connecting to the UNSTRUCTURED API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ccc8547c6539d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:43:55.575099Z",
     "start_time": "2024-07-03T13:43:55.564950Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "5ccc8547c6539d3b",
    "outputId": "81865be5-be71-4a53-e41f-ac1c75f50f45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('./.env') # replace with the path to your .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5cefe",
   "metadata": {},
   "source": [
    "### Put Unstructure Data in AWS S3\n",
    "\n",
    "![Image Alt Text](./image/files_in_s3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-80mXole4qAe",
   "metadata": {
    "id": "-80mXole4qAe"
   },
   "source": [
    "### Create index in Singdata Lakehouse\n",
    "\n",
    "Before we build the unstructured data preprocessing pipeline, let's create a Singdata Lakehouse schema and a table in it to store the processed data.\n",
    "\n",
    "For an example of a schema, please refer to [Unstructured documentation](https://docs.unstructured.io/api-reference/ingest/destination-connector/singlestore#singlestore-table-schema). If you'll be using the schema from the documentation, make sure that the `dims` value for the embeddings matches the number of dimensions of the embeddings model you choose to use. In this example it's set to {embeddings_dimensions}, but your embedding model may produce vectors of a different dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d199049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the table names to use for storing the data in Lakehouse.\n",
    "# index_and_table_prefix = \"base_512_\"\n",
    "# raw_table_name = f\"{index_and_table_prefix}yunqi_raw_elements\"\n",
    "# silver_table_name = f\"{index_and_table_prefix}yunqi_elements\"\n",
    "# embeddings_dimensions = 768\n",
    "# chunk_max_characters =512\n",
    "# chunk_overlap = 200\n",
    "# embedding_provider = \"huggingface\"\n",
    "# embedding_model_name = \"BAAI/bge-base-zh-v1.5\"\n",
    "\n",
    "\n",
    "index_and_table_prefix = \"m3_1024_2048_20250510_\"\n",
    "raw_table_name = f\"{index_and_table_prefix}yunqi_raw_elements\"\n",
    "silver_table_name = f\"{index_and_table_prefix}yunqi_elements\"\n",
    "embeddings_dimensions = 1024\n",
    "chunk_max_characters =2048\n",
    "chunk_overlap = 512\n",
    "embedding_provider = \"huggingface\"\n",
    "embedding_model_name = \"BAAI/bge-m3\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the connection parameter to Singdata Lakehouse.\n",
    "_username = os.getenv(\"cz_username\")\n",
    "_password = os.getenv(\"cz_password\")\n",
    "_service = os.getenv(\"cz_service\")\n",
    "_instance = os.getenv(\"cz_instance\")\n",
    "_workspace = os.getenv(\"cz_workspace\")\n",
    "_schema = os.getenv(\"cz_schema\")\n",
    "_vcluster = os.getenv(\"cz_vcluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191ee142",
   "metadata": {},
   "source": [
    "This silver layer table is designed to store various metadata and content related to files and elements. The two indexes optimize the performance of specific queries:\n",
    "\n",
    "The inverted text index enhances full-text search capabilities, making it easier to find records based on text content.\n",
    "\n",
    "The embeddings vector index optimizes similarity searches on vector data, which is useful for tasks that involve comparing and finding similar elements based on their embeddings.\n",
    "\n",
    "Benefits for RAG (Retrieval-Augmented Generation) Application Development:\n",
    "\n",
    "Enhanced Search Efficiency: By supporting both inverted and vector searches, this table allows RAG applications to efficiently retrieve relevant information based on both text content and semantic similarity. This enhances the model's ability to find and generate contextually relevant responses.\n",
    "\n",
    "Improved Accuracy: The combination of full-text and similarity searches ensures that RAG applications can access a broader range of relevant data, improving the accuracy and relevance of generated content.\n",
    "\n",
    "Scalability: With optimized indexes, the table can handle large volumes of data and perform searches quickly, supporting the scalability needs of RAG applications.\n",
    "\n",
    "Simplified Architecture: Combining inverted text and vector search capabilities in a single table eliminates the need for separate text and vector search databases. This simplifies maintenance, reduces operational overhead, and improves development efficiency.\n",
    "\n",
    "Data Consistency: Reducing the number of data replicas from three to one enhances data consistency, minimizes data duplication, and reduces the need for data synchronization and movement.\n",
    "\n",
    "Overall, these indexes ensure that searches and retrievals on the text and embeddings fields are performed efficiently, supporting quick and accurate query results, which are crucial for the development of effective and streamlined RAG applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1164b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema to use for storing the data in Singdata Lakehouse.\n",
    "raw_table_ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {_schema}.{raw_table_name} (\n",
    "    id STRING, -- Auto-increment sequence\n",
    "    record_locator STRING,\n",
    "    type STRING,\n",
    "    record_id STRING, -- Record identifier from the data source (e.g., record locator in connector metadata)\n",
    "    element_id STRING, -- Unique identifier for the element (SHA-256 or UUID)\n",
    "    filetype STRING, -- File type (e.g., PDF, DOCX, EML, etc.)\n",
    "    file_directory STRING, -- Directory where the file is located\n",
    "    filename STRING, -- File name\n",
    "    last_modified TIMESTAMP, -- Last modified time of the file\n",
    "    languages STRING, -- Document language, supports a list of multiple languages\n",
    "    page_number STRING, -- Page number (applicable for PDF, DOCX, etc.)\n",
    "    text STRING, -- Extracted text content\n",
    "    embeddings VECTOR({embeddings_dimensions}), -- Vector data\n",
    "    parent_id STRING, -- Parent element ID, used to represent element hierarchy\n",
    "    is_continuation BOOLEAN, -- Whether it is a continuation of the previous element (used in chunking)\n",
    "    orig_elements STRING, -- Original element in JSON format (used to store the complete element structure)\n",
    "    element_type STRING, -- Element type (e.g., NarrativeText, Title, Table, etc.)\n",
    "    coordinates STRING, -- Element coordinates (stored in JSONB format)\n",
    "    link_texts STRING, -- Added field: Link text\n",
    "    link_urls STRING, -- Added field: Link URL\n",
    "    email_message_id STRING, -- Added field: Email message ID\n",
    "    sent_from STRING, -- Added field: Sender\n",
    "    sent_to STRING, -- Added field: Recipient\n",
    "    subject STRING, -- Added field: Subject\n",
    "    url STRING, -- Added field: URL\n",
    "    version STRING, -- Added field: Version\n",
    "    date_created TIMESTAMP, -- Added field: Creation date\n",
    "    date_modified TIMESTAMP, -- Added field: Modification date\n",
    "    date_processed TIMESTAMP, -- Added field: Processing date\n",
    "    text_as_html STRING, -- Added field: Text in HTML format\n",
    "    emphasized_text_contents STRING,\n",
    "    emphasized_text_tags STRING,\n",
    "    documents_original_source STRING, -- Added field: Document source\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "silver_table_ddl = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {_schema}.{silver_table_name} (\n",
    "    id STRING, -- Auto-increment sequence\n",
    "    record_locator STRING,\n",
    "    type STRING,\n",
    "    record_id STRING, -- Record identifier from the data source (e.g., record locator in connector metadata)\n",
    "    element_id STRING, -- Unique identifier for the element (SHA-256 or UUID)\n",
    "    filetype STRING, -- File type (e.g., PDF, DOCX, EML, etc.)\n",
    "    file_directory STRING, -- Directory where the file is located\n",
    "    filename STRING, -- File name\n",
    "    last_modified TIMESTAMP, -- Last modified time of the file\n",
    "    languages STRING, -- Document language, supports a list of multiple languages\n",
    "    page_number STRING, -- Page number (applicable for PDF, DOCX, etc.)\n",
    "    text STRING, -- Extracted text content\n",
    "    embeddings vector({embeddings_dimensions}), -- Vector data\n",
    "    parent_id STRING, -- Parent element ID, used to represent element hierarchy\n",
    "    is_continuation BOOLEAN, -- Whether it is a continuation of the previous element (used in chunking)\n",
    "    orig_elements STRING, -- Original element in JSON format (used to store the complete element structure)\n",
    "    element_type STRING, -- Element type (e.g., NarrativeText, Title, Table, etc.)\n",
    "    coordinates STRING, -- Element coordinates (stored in JSONB format)\n",
    "    link_texts STRING, -- Added field: Link text\n",
    "    link_urls STRING, -- Added field: Link URL\n",
    "    email_message_id STRING, -- Added field: Email message ID\n",
    "    sent_from STRING, -- Added field: Sender\n",
    "    sent_to STRING, -- Added field: Recipient\n",
    "    subject STRING, -- Added field: Subject\n",
    "    url STRING, -- Added field: URL\n",
    "    version STRING, -- Added field: Version\n",
    "    date_created TIMESTAMP, -- Added field: Creation date\n",
    "    date_modified TIMESTAMP, -- Added field: Modification date\n",
    "    date_processed TIMESTAMP, -- Added field: Processing date\n",
    "    text_as_html STRING, -- Added field: Text in HTML format\n",
    "    emphasized_text_contents STRING,\n",
    "    emphasized_text_tags STRING,\n",
    "    documents_source STRING, -- Added field: Document source\n",
    "    INDEX {index_and_table_prefix}inverted_text_index_yunqi_cn (text) INVERTED  PROPERTIES('analyzer'='unicode'),\n",
    "    INDEX {index_and_table_prefix}embeddings_vec_index_yunqi_cn(embeddings) USING vector properties (\n",
    "        \"scalar.type\" = \"f32\",\n",
    "        \"distance.function\" = \"cosine_distance\")\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "clean_transformation_data_sql = f\"\"\"\n",
    "INSERT overwrite {_schema}.{silver_table_name}\n",
    "SELECT \n",
    "    id, \n",
    "    record_locator, \n",
    "    type, \n",
    "    record_id, \n",
    "    element_id, \n",
    "    filetype, \n",
    "    file_directory, \n",
    "    filename, \n",
    "    last_modified, \n",
    "    languages, \n",
    "    page_number, \n",
    "    text, \n",
    "    CAST(embeddings AS VECTOR({embeddings_dimensions})) AS embeddings, \n",
    "    parent_id, \n",
    "    is_continuation, \n",
    "    orig_elements, \n",
    "    element_type, \n",
    "    coordinates, \n",
    "    link_texts, \n",
    "    link_urls, \n",
    "    email_message_id, \n",
    "    sent_from, \n",
    "    sent_to, \n",
    "    subject, \n",
    "    url, \n",
    "    version, \n",
    "    date_created, \n",
    "    date_modified, \n",
    "    date_processed, \n",
    "    text_as_html,\n",
    "    emphasized_text_contents, \n",
    "    emphasized_text_tags \n",
    "FROM {_schema}.{raw_table_name};\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a45736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to create the connection to Singdata Lakehouse.\n",
    "from clickzetta.connector import connect\n",
    "import pandas as pd\n",
    "def get_connection(password, username, service, instance, workspace, schema, vcluster):\n",
    "    connection = connect(\n",
    "        password=password,\n",
    "        username=username,\n",
    "        service=service,\n",
    "        instance=instance,\n",
    "        workspace=workspace,\n",
    "        schema=schema,\n",
    "        vcluster=vcluster)\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the connection to Singdata Lakehouse.\n",
    "conn = get_connection(password=_password, username=_username, service=_service, instance=_instance, workspace=_workspace, schema=_schema, vcluster=_vcluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7fcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute SQL statements\n",
    "def excute_sql(conn,sql_statement: str):\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        stmt = sql_statement\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ad155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_tables:\n",
    "    excute_sql(conn,f\"DROP TABLE IF EXISTS {_schema}.{raw_table_name}\")\n",
    "    # excute_sql(conn,f\"DROP TABLE IF EXISTS {_schema}.{silver_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba69a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Table in Singdata Lakehouse\n",
    "excute_sql(conn, raw_table_ddl)\n",
    "excute_sql(conn, silver_table_ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sxDcltMB5i17",
   "metadata": {
    "id": "sxDcltMB5i17"
   },
   "source": [
    "Creating a database may take a few seconds. Let's check the status. We want to make sure that it says `healthy` before we begin writing into it.\n",
    "\n",
    "![Image Alt Text](./image/unstructured_tables.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8X_GQ32GQnI",
   "metadata": {
    "id": "a8X_GQ32GQnI"
   },
   "source": [
    "### PDFs/Images/Emails ingestion and preprocessing pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EQ0GXjYMGUqO",
   "metadata": {
    "id": "EQ0GXjYMGUqO"
   },
   "source": [
    "Unstructured ingestion and transformation pipeline is compiled from a number of necessary configs. These don't have to be in the exact same order.\n",
    "\n",
    "* `ProcessorConfig`: defines general processing behavior\n",
    "\n",
    "* `S3IndexerConfig`, `S3DownloaderConfig`, `S3ConnectionConfig`: control data ingestion from S3, including source location, and authentication options.\n",
    "\n",
    "* `PartitionerConfig`: describes partitioning behavior. Here we only set up authentication for the Unstructured API, but you can also control [partitioning parameters](https://docs.unstructured.io/api-reference/ingest/ingest-configuration/partition-configuration) such as partitioning strategy through this config. We're going with the defaults.  \n",
    "\n",
    "* `ChunkerConfig`: defines the chunking strategy, and chunk sizes.\n",
    "\n",
    "* `EmbedderConfig`: sets up connection to an embedding model provider to generate embeddings for data chunks.\n",
    "\n",
    "* `ClickzettaConnectionConfig`, `ClickzettaUploadStagerConfig`, `ClickzettaUploaderConfig`: control the final step of the pipeline - data loading into Singdata Lakehouse RAW table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99881631767c71e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:44:10.587954Z",
     "start_time": "2024-07-03T13:44:04.335563Z"
    },
    "id": "99881631767c71e2"
   },
   "outputs": [],
   "source": [
    "from unstructured_ingest.interfaces import ProcessorConfig\n",
    "from unstructured_ingest.pipeline.pipeline import Pipeline\n",
    "from unstructured_ingest.processes.chunker import ChunkerConfig\n",
    "from unstructured_ingest.processes.connectors.fsspec.s3 import (\n",
    "    S3ConnectionConfig,\n",
    "    S3DownloaderConfig,\n",
    "    S3IndexerConfig,\n",
    "    S3AccessConfig,\n",
    ")\n",
    "from unstructured_ingest.processes.embedder import EmbedderConfig\n",
    "from unstructured_ingest.processes.partitioner import PartitionerConfig\n",
    "\n",
    "from unstructured_ingest.processes.connectors.sql.clickzetta import (\n",
    "    ClickzettaConnectionConfig,\n",
    "    ClickzettaAccessConfig,\n",
    "    ClickzettaUploadStagerConfig,\n",
    "    ClickzettaUploaderConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8397d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /Users/liangmo/.cache/unstructured/ingest/pipeline/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018807c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"AWS_S3_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc718af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_configs(\n",
    "\n",
    "    context=ProcessorConfig(\n",
    "        verbose=True,\n",
    "        tqdm=True,\n",
    "        num_processes=20,\n",
    "    ),\n",
    "\n",
    "    indexer_config=S3IndexerConfig(remote_url=os.getenv(\"AWS_S3_NAME\"), recursive=True, file_glob=\"**/*.md\" ),\n",
    "    downloader_config=S3DownloaderConfig(),\n",
    "    source_connection_config=S3ConnectionConfig(\n",
    "        access_config=S3AccessConfig(\n",
    "            key=os.getenv(\"AWS_KEY\"),\n",
    "            secret=os.getenv(\"AWS_SECRET\"))\n",
    "    ),\n",
    "\n",
    "    partitioner_config=PartitionerConfig(\n",
    "        partition_by_api=False,\n",
    "        api_key=os.getenv(\"UNSTRUCTURED_API_KEY\"),\n",
    "        partition_endpoint=os.getenv(\"UNSTRUCTURED_URL\"),\n",
    "        strategy=\"hi_res\",\n",
    "        additional_partition_args={\n",
    "            \"split_pdf_page\": True,\n",
    "            \"split_pdf_allow_failed\": True,\n",
    "            \"split_pdf_concurrency_level\": 15\n",
    "        }\n",
    "    ),\n",
    "\n",
    "    chunker_config=ChunkerConfig(\n",
    "        chunking_strategy=\"by_title\",\n",
    "        chunk_max_characters=chunk_max_characters,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        chunk_combine_text_under_n_chars=200,\n",
    "    ),\n",
    "\n",
    "    embedder_config=EmbedderConfig(\n",
    "        embedding_provider = embedding_provider,\n",
    "        embedding_model_name = embedding_model_name,\n",
    "    ),\n",
    "\n",
    "    destination_connection_config=ClickzettaConnectionConfig(\n",
    "        access_config=ClickzettaAccessConfig(password=_password),\n",
    "        username=_username,\n",
    "        service=_service,\n",
    "        instance=_instance,\n",
    "        workspace=_workspace,\n",
    "        schema=_schema,\n",
    "        vcluster=_vcluster,\n",
    "    ),\n",
    "    stager_config=ClickzettaUploadStagerConfig(),\n",
    "    uploader_config=ClickzettaUploaderConfig(table_name=raw_table_name, documents_original_source=\"https://yunqi.tech/documents\"),\n",
    ")\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02177bb7",
   "metadata": {},
   "source": [
    "### Clean/Transformation RAW table and Insert into Silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7cc37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could excute more SQLs to clean and transform data before insert into Silver table.ã\n",
    "excute_sql(conn, clean_transformation_data_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mSCldyUAGmbF",
   "metadata": {
    "id": "mSCldyUAGmbF"
   },
   "source": [
    "### Check the RAG data Ready outputs\n",
    "\n",
    "Let's connect to the Singdata Lakehouse. In the logs to the previous cell, you can see how many elements have been uploaded during the Upload Step for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16060c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rag_ready_data(conn,  num_results: int = 5):\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        stmt = f\"\"\"\n",
    "            SELECT\n",
    "                *\n",
    "            FROM {silver_table_name}\n",
    "            LIMIT {num_results}\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]  # Get column names from cursor description\n",
    "        rag_ready_data_df = pd.DataFrame(results, columns=columns)\n",
    "    return rag_ready_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_ready_data_df = get_rag_ready_data(conn)\n",
    "rag_ready_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d83c30",
   "metadata": {},
   "source": [
    "Or you could check the data Via Singdata Lakehouse Studio.\n",
    "\n",
    "\n",
    "![Image Alt Text](./image/unstructured_table_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb4febe23f1832",
   "metadata": {
    "id": "b3fb4febe23f1832"
   },
   "source": [
    "### Retrieve relevant documents from Singdata Lakehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393cb93e9706feba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T18:15:20.149136Z",
     "start_time": "2024-07-08T18:15:20.143790Z"
    },
    "id": "393cb93e9706feba"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def get_embedding(query):\n",
    "    model = SentenceTransformer(embedding_model_name)\n",
    "    return model.encode(query, normalize_embeddings=True)\n",
    "\n",
    "def retrieve_documents(conn, query: str, num_results: int = 10):\n",
    "\n",
    "    embedding = get_embedding(query)\n",
    "    embedding_list = embedding.tolist()\n",
    "    embedding_json = json.dumps(embedding_list)\n",
    "\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        stmt = f\"\"\"\n",
    "            WITH \n",
    "            vector_embedding_result AS (\n",
    "            SELECT\n",
    "                \"vector_embedding\" as retrieve_method,\n",
    "                record_locator,\n",
    "                type,\n",
    "                filename,\n",
    "                text,\n",
    "                orig_elements,\n",
    "                cosine_distance(embeddings, cast({embedding_list} as vector({embeddings_dimensions}))) AS score\n",
    "            FROM {silver_table_name}\n",
    "            ORDER BY score ASC\n",
    "            LIMIT {num_results} \n",
    "            )\n",
    "            SELECT    *  FROM      vector_embedding_result\n",
    "           \n",
    "            ORDER by score ASC;\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]  # Get column names from cursor description\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c60264",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embedding(\"What is gartner leadership vision for digital tech?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999c70db27f0a19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T18:15:21.736297Z",
     "start_time": "2024-07-08T18:15:20.686070Z"
    },
    "id": "5999c70db27f0a19",
    "outputId": "9081a8db-d7c9-45b8-e9f4-41c940c254df"
   },
   "outputs": [],
   "source": [
    "query_text = \"åå»ºç´¢å¼çè¯­æ³æ¯ä»ä¹ï¼\"\n",
    "retrieve_documents_df = retrieve_documents(conn, query_text)\n",
    "retrieve_documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row__text = retrieve_documents_df.iloc[4]['text']\n",
    "print(first_row__text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = retrieve_documents_df.iloc[0]['filename']\n",
    "with conn.cursor() as cur:\n",
    "\n",
    "        stmt = f\"\"\"\n",
    "            WITH \n",
    "            results AS (\n",
    "            SELECT\n",
    "                record_locator,\n",
    "                type,\n",
    "                filename,\n",
    "                text,\n",
    "                orig_elements,\n",
    "            FROM {silver_table_name}\n",
    "            WHERE filename = \"{filename}\"\n",
    "            )\n",
    "            SELECT    *  FROM      results;\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]  # Get column names from cursor description\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ce8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\".join(df[\"text\"].astype(str).tolist())\n",
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc437868b78a41b0",
   "metadata": {
    "id": "bc437868b78a41b0"
   },
   "outputs": [],
   "source": [
    "def match_all_documents(conn, query: str, num_results: int = 1):\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        stmt = f\"\"\"\n",
    "            WITH \n",
    "            scalar_match_all_result AS (\n",
    "            SELECT\n",
    "                \"scalar_match_all\" as retrieve_method,\n",
    "                record_locator,\n",
    "                type,\n",
    "                filename,\n",
    "                text,\n",
    "                orig_elements,\n",
    "                -100 AS score\n",
    "            FROM {silver_table_name}\n",
    "            WHERE match_all(\n",
    "                    text,\n",
    "                    \"{query}\",\n",
    "                    map(\"analyzer\", \"unicode\")\n",
    "                    )\n",
    "            ORDER BY score ASC\n",
    "            LIMIT {num_results} \n",
    "            )\n",
    "            SELECT    *  FROM      scalar_match_all_result\n",
    "            ORDER by score ASC;\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]  # Get column names from cursor description\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d41fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_all_documents_df = match_all_documents(conn,query_text)\n",
    "match_all_documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11220e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_any_documents(conn, query: str, num_results: int = 5):\n",
    "    with conn.cursor() as cur:\n",
    "\n",
    "        stmt = f\"\"\"\n",
    "            WITH \n",
    "            scalar_match_any_result AS (\n",
    "            SELECT\n",
    "                \"scalar_match_any\" as retrieve_method,\n",
    "                record_locator,\n",
    "                type,\n",
    "                filename,\n",
    "                text,\n",
    "                orig_elements,\n",
    "                0 AS score\n",
    "            FROM {silver_table_name}\n",
    "            WHERE match_any(\n",
    "                    text,\n",
    "                    \"{query}\",\n",
    "                    map(\"analyzer\", \"unicode\")\n",
    "                    )\n",
    "            ORDER BY score ASC\n",
    "            LIMIT {num_results} \n",
    "            )\n",
    "            SELECT    *  FROM      scalar_match_any_result\n",
    "            ORDER by score ASC;\n",
    "        \"\"\"\n",
    "\n",
    "        cur.execute(stmt)\n",
    "\n",
    "        results = cur.fetchall()\n",
    "        columns = [desc[0] for desc in cur.description]  # Get column names from cursor description\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354fd3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_any_documents_df = match_any_documents(conn,query_text)\n",
    "match_any_documents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112cf846",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([retrieve_documents_df, match_all_documents_df, match_any_documents_df], ignore_index=True)\n",
    "merged_df = merged_df.sort_values(by='score', ascending=True)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d90305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "\n",
    "# Define the rerank function\n",
    "def rerank_texts(query, texts, model_name=\"BAAI/bge-reranker-v2-m3\", normalize=True):\n",
    "    \"\"\"\n",
    "    Rerank a list of texts based on their relevance to a given query using the specified reranker model.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The query string.\n",
    "    - texts: List of texts to be reranked.\n",
    "    - model_name: The name of the reranker model to use.\n",
    "    - normalize: Whether to normalize the scores to the [0, 1] range using the sigmoid function.\n",
    "\n",
    "    Returns:\n",
    "    - A list of reranked texts.\n",
    "    - A list of corresponding scores.\n",
    "    \"\"\"\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare input pairs [query, text]\n",
    "    pairs = [[query, text] for text in texts]\n",
    "    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "    # Get relevance scores\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        scores = outputs.logits.view(-1).cpu().numpy()\n",
    "\n",
    "    # Normalize scores to [0, 1] if required\n",
    "    if normalize:\n",
    "        scores = 1 / (1 + np.exp(-scores))\n",
    "\n",
    "    # Combine texts with scores and sort by score in descending order\n",
    "    scored_texts = list(zip(texts, scores))\n",
    "    scored_texts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Separate the sorted texts and scores\n",
    "    sorted_texts, sorted_scores = zip(*scored_texts)\n",
    "\n",
    "    return list(sorted_texts), list(sorted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# query = \"Which session is presented by Ajeeta Malhotra and Amol Nadkarni?\"\n",
    "# query = \"What is gartner leadership vision for digital tech?\"\n",
    "sorted_texts, sorted_scores = rerank_texts(query_text, merged_df[\"text\"].tolist())\n",
    "\n",
    "# Update DataFrame with reranked texts and scores\n",
    "merged_df[\"reranked_text\"] = sorted_texts\n",
    "merged_df[\"rerank_score\"] = sorted_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first row of the DataFrame, which get the highest rerank_score\n",
    "first_row_reranked_text = merged_df.iloc[0]['text']\n",
    "print(first_row_reranked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34eaa67",
   "metadata": {},
   "source": [
    "### Summary Benefits for RAG Application Development\n",
    "\n",
    "![Image Alt Text](./image/UnstructuredDataPipelineBenifits.png)\n",
    "\n",
    "\n",
    "**Enhanced Search Efficiency:** \n",
    "- By supporting both inverted and vector searches, this table allows RAG applications to efficiently retrieve relevant information based on both text content and semantic similarity. This enhances the model's ability to find and generate contextually relevant responses.\n",
    "\n",
    "**Improved Accuracy:** \n",
    "- The combination of full-text and similarity searches ensures that RAG applications can access a broader range of relevant data, improving the accuracy and relevance of generated content.\n",
    "\n",
    "**Scalability:** \n",
    "- With optimized indexes, the table can handle large volumes of data and perform searches quickly, supporting the scalability needs of RAG applications.\n",
    "\n",
    "**Simplified Architecture:** \n",
    "- Combining inverted text and vector search capabilities in a single table eliminates the need for separate text and vector search databases. This simplifies maintenance, reduces operational overhead, and improves development efficiency.\n",
    "\n",
    "**Data Consistency:** \n",
    "- Reducing the number of data replicas from three to one enhances data consistency, minimizes data duplication, and reduces the need for data synchronization and movement.\n",
    "\n",
    "Overall, this Singdata Lakehouse architecture reduces operational complexity, enhances data consistency, and improves development efficiency, making it ideal for effective RAG application development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb = \"ClickZetta\\äºå¨\\Singdataä¹é´çå³ç³»:ClickZettaæ¯äºå¨ãSingdataçææ¯åçãäºå¨æ¯ClickZettaçä¸­å½åä¸å¬å¸ï¼Singdataæ¯ClickZettaçæµ·å¤åä¸å¬å¸ãæä»¥å¨ä¸åºååä¸åææ¯çæ¶åï¼ä¸èæ¯ç­åçãå¦å¤äºå¨ç»å¸¸ä¹ä¼ç¨å°Yunqiçæ±è¯­æ¼é³åç§°ã\"\n",
    "embedded_kb = get_embedding(kb)\n",
    "embedded_kb = embedded_kb.tolist()\n",
    "add_kb_sql = f\"\"\"\n",
    "INSERT INTO quick_start.clickzetta_doc_kb.yunqi_elements (\n",
    "  id, type, record_id, element_id, filetype, last_modified, languages, text, embeddings, date_created, date_modified, date_processed\n",
    ") VALUES (\n",
    "  uuid(), 'UserInput', uuid(), uuid(), 'text', CURRENT_TIMESTAMP, '[\"zh-cn\"]',\n",
    "  '{kb}',\n",
    "  CAST('{embedded_kb}' AS vector(float,{embeddings_dimensions})), CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28416139",
   "metadata": {},
   "outputs": [],
   "source": [
    "with conn.cursor() as cur:\n",
    "        cur.execute(add_kb_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-m3\")\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy person\",\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities.shape)\n",
    "# [4, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93403c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "unstructured311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
